{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concealed Pistol Bounding Boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for neural network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchsummary import summary\n",
    "\n",
    "#imports for vision\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.ops import distance_box_iou_loss\n",
    "\n",
    "#imports for preparing dataset\n",
    "import os\n",
    "import zipfile\n",
    "#from google.colab import files\n",
    "#from google.colab import drive\n",
    "\n",
    "#imports for visualizations\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from custom_gun_dataset import CustomDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying a transformation to the entire dataset, standardizing it\n",
    "#reshapes the image to guarantee a 384 size\n",
    "#grayscales the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((384, 384)), \n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "#relative directory path\n",
    "dataset_dir = '../Data/CombinedData'\n",
    "dataset = CustomDataset(root_directory = dataset_dir, transform = transform, categories = ['with gun'])\n",
    "\n",
    "#initialize train, validation, and test sets\n",
    "train_size = int(0.8 * len(dataset))              #80% for training\n",
    "val_size = int(0.1 * len(dataset))                #10% for validation\n",
    "test_size = len(dataset) - train_size - val_size  #10% (remainder) for test\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "#print sizes of datasets\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Valid size: {len(val_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")\n",
    "\n",
    "#set the dataloaders to use the datasets\n",
    "batch_size = 4\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#variable to define number of classes\n",
    "num_classes = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting device to GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting device to gpu if availible\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "#confirm device\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting a grid of a single batch\n",
    "def show_batch(dataLoader):\n",
    "    for images, class_label, boxes in dataLoader:\n",
    "        grid_img = make_grid(images, nrow=4, padding=2).permute(1, 2, 0)  # Create image grid\n",
    "        fig, ax = plt.subplots(figsize=(15, 10))\n",
    "        \n",
    "        ax.imshow(grid_img)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "        print(boxes)\n",
    "        # Compute the scale factor to map box coordinates to the grid\n",
    "        img_width = images.shape[3]  # Image width\n",
    "        img_height = images.shape[2]  # Image height\n",
    "        grid_width, grid_height = grid_img.shape[1], grid_img.shape[0]\n",
    "        \n",
    "        scale_x = grid_width / (4 * img_width)  # Scale for width\n",
    "        scale_y = grid_height / ((batch_size // 4) * img_height)  # Scale for height\n",
    "\n",
    "        # Loop through each image and its bounding boxes\n",
    "        print(len(boxes), len(boxes[0]))\n",
    "        for i in range(batch_size):\n",
    "            row = i // 4  # Compute row index\n",
    "            col = i % 4  # Compute column index\n",
    "\n",
    "\n",
    "            x1, y1, x2, y2 = boxes[0][i],boxes[1][i],boxes[2][i],boxes[3][i]\n",
    "            #print(x1, y1, x2, y2)\n",
    "\n",
    "            #x *= img_width\n",
    "            #y *= img_height\n",
    "            #width *= img_width\n",
    "            #height *= img_height\n",
    "            \n",
    "            # Adjust bounding box coordinates to fit the grid  \n",
    "            x1 = (x1 ) + (col  * img_width) \n",
    "            y1 = (y1 ) + (row  * img_height)\n",
    "            x2 = (x2 ) + (col  * img_width) \n",
    "            y2 = (y2 ) + (row  * img_height)\n",
    "            width = (x2 - x1)\n",
    "            height = (y2 - y1)\n",
    "            \n",
    "            # Draw the rectangle\n",
    "            rect = patches.Rectangle((x1, y1), width, height, linewidth=2, edgecolor='red', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        plt.show()\n",
    "        break  # Show only one batch\n",
    "\n",
    "show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition for a CNN\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        #model takes input of 384 x 384 x 1\n",
    "        #make sure in_channels aligns with out_channels from the previous layer\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.bn7 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.conv8 = nn.Conv2d(in_channels = 256, out_channels = 128, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.bn8 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv9 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.bn9 = nn.BatchNorm2d(128)\n",
    "\n",
    "        #16 x 16 x 128 comes from sizing, the pool in each layer cut dimensionality in half, 256 is out channels\n",
    "        self.fc1 = nn.Linear(in_features = (24 * 24 * 128), out_features = 64)\n",
    "        self.fc2 = nn.Linear(in_features = 64, out_features = 32)\n",
    "        #self.fc3 = nn.Linear(in_features = 32, out_features = 32)\n",
    "        #self.fc4 = nn.Linear(in_features = 32, out_features = 32)\n",
    "        self.fc_class = nn.Linear(in_features = 32, out_features = num_classes)\n",
    "        self.fc_bbox = nn.Linear(in_features = 32, out_features = 4)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        #forward pass first block, no pool\n",
    "        output = self.conv1(input)\n",
    "        output = self.bn1(output)\n",
    "        output = self.leaky_relu(output)\n",
    "        #output_pool_1 = self.pool(output)\n",
    "\n",
    "        #forward pass second block, pool\n",
    "        output = self.conv2(output)\n",
    "        output = self.bn2(output)\n",
    "        output = self.leaky_relu(output)\n",
    "        output_pool_2 = self.pool(output)\n",
    "\n",
    "        #forward pass third block, no pool\n",
    "        output = self.conv3(output_pool_2)\n",
    "        output = self.bn3(output)\n",
    "        output = self.leaky_relu(output)\n",
    "        #output_pool_3 = self.pool(output)\n",
    "\n",
    "        #skip connection, connect 1-3\n",
    "        #skip1 = self.pool(input) #downsample\n",
    "        #if skip1.shape[1] != output.shape[1]:  # if channels differ, adjust\n",
    "        #    skip1 = nn.functional.pad(skip1, (0, 0, 0, 0, 0, output.shape[1] - skip1.shape[1]))\n",
    "        #output += skip1\n",
    "        output_skip_1 = output\n",
    "\n",
    "        #forward pass fourth block, pool\n",
    "        output = self.conv4(output_skip_1)\n",
    "        output = self.bn4(output)\n",
    "        output = self.leaky_relu(output)\n",
    "        output_pool_4 = self.pool(output)\n",
    "\n",
    "        #forward pass fifth block, no pool\n",
    "        output = self.conv5(output_pool_4)\n",
    "        output = self.bn5(output)\n",
    "        output = self.leaky_relu(output)\n",
    "        #output_pool_5 = self.pool(output)\n",
    "\n",
    "        #forward pass sixth block, pool\n",
    "        output = self.conv6(output)\n",
    "        output = self.bn6(output)\n",
    "        output = self.leaky_relu(output)\n",
    "        output_pool_6 = self.pool(output)\n",
    "\n",
    "        #skip connection, connect 4-6\n",
    "        skip2 = self.pool(self.pool(output_skip_1)) #downsample\n",
    "        if skip2.shape[1] != output_pool_6.shape[1]:  # if channels differ, adjust\n",
    "            skip2 = nn.functional.pad(skip2, (0, 0, 0, 0, 0, output_pool_6.shape[1] - skip2.shape[1]))\n",
    "        output_pool_6 += skip2\n",
    "        output_skip_2 = output_pool_6\n",
    "\n",
    "        #forward pass seventh block, no pool\n",
    "        output = self.conv7(output_pool_6)\n",
    "        output = self.bn7(output)\n",
    "        output = self.leaky_relu(output)\n",
    "\n",
    "        #forward pass eigth block, pool\n",
    "        output = self.conv8(output)\n",
    "        output = self.bn8(output)\n",
    "        output = self.leaky_relu(output)\n",
    "        output_pool_8 = self.pool(output)\n",
    "\n",
    "        #forward pass ninth block, no pool\n",
    "        output = self.conv9(output_pool_8)\n",
    "        output = self.bn9(output)\n",
    "        output = self.leaky_relu(output)\n",
    "\n",
    "        #skip connection, connect 6-9\n",
    "        skip3 = self.pool(output_skip_2) # downsample input to match spatial dims\n",
    "        if skip3.shape[1] != output.shape[1]:  # if channels differ, adjust\n",
    "            skip3 = nn.functional.pad(skip3, (0,0,0,0, 0, output.shape[1] - skip3.shape[1]))\n",
    "        output += skip3\n",
    "        output_skip_3 = output\n",
    "\n",
    "        #forward pass flattening\n",
    "        output = output.view(-1, 128 * 24 * 24)\n",
    "\n",
    "        #forward pass fully connected layers\n",
    "        output = self.fc1(output)\n",
    "        output = self.leaky_relu(output)\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        output = self.fc2(output)\n",
    "        output = self.leaky_relu(output)\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        #output = self.fc3(output)\n",
    "        #output = self.leaky_relu(output)\n",
    "        #output = self.dropout(output)\n",
    "\n",
    "        #output = self.fc4(output)\n",
    "        #output = self.leaky_relu(output)\n",
    "        #output = self.dropout(output)\n",
    "\n",
    "        output_class = self.fc_class(output)\n",
    "        \n",
    "        output_bbox = self.sigmoid(output)\n",
    "        output_bbox = self.fc_bbox(output_bbox)\n",
    "\n",
    "        return output_class, output_bbox\n",
    "\n",
    "model = Network().to(device)\n",
    "\n",
    "#channels, height, width\n",
    "summary(model,(1, 384, 384))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Loss and Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bbox_ciou(box1, box2):\n",
    "    # Intersection\n",
    "    x1 = torch.min(box1[:, 0], box1[:, 2])\n",
    "    y1 = torch.min(box1[:, 1], box1[:, 3])\n",
    "    x2 = torch.max(box1[:, 0], box1[:, 2])\n",
    "    y2 = torch.max(box1[:, 1], box1[:, 3]) \n",
    "    pred = torch.stack([x1, y1, x2, y2], dim=1)\n",
    "\n",
    "    inter_x1 = torch.max(pred[:, 0], box2[:, 0])\n",
    "    inter_y1 = torch.max(pred[:, 1], box2[:, 1])\n",
    "    inter_x2 = torch.min(pred[:, 2], box2[:, 2])\n",
    "    inter_y2 = torch.min(pred[:, 3], box2[:, 3])\n",
    "\n",
    "    inter_area = (inter_x2 - inter_x1).clamp(0) * (inter_y2 - inter_y1).clamp(0)\n",
    "\n",
    "    # Areas\n",
    "    area1 = (pred[:, 2] - pred[:, 0]).clamp(0) * (pred[:, 3] - pred[:, 1]).clamp(0)\n",
    "    area2 = (box2[:, 2] - box2[:, 0]).clamp(0) * (box2[:, 3] - box2[:, 1]).clamp(0)\n",
    "\n",
    "    union_area = area1 + area2 - inter_area + 1e-6\n",
    "    iou = inter_area / union_area\n",
    "\n",
    "    # Centers\n",
    "    # b1_center_x = (pred[:, 0] + pred[:, 2]) / 2\n",
    "    # b1_center_y = (pred[:, 1] + pred[:, 3]) / 2\n",
    "    # b2_center_x = (box2[:, 0] + box2[:, 2]) / 2\n",
    "    # b2_center_y = (box2[:, 1] + box2[:, 3]) / 2\n",
    "\n",
    "    # center_dist_sq = (b1_center_x - b2_center_x) ** 2 + (b1_center_y - b2_center_y) ** 2\n",
    "\n",
    "    # # Enclosing box\n",
    "    # enc_x1 = torch.min(pred[:, 0], box2[:, 0])\n",
    "    # enc_y1 = torch.min(pred[:, 1], box2[:, 1])\n",
    "    # enc_x2 = torch.max(pred[:, 2], box2[:, 2])\n",
    "    # enc_y2 = torch.max(pred[:, 3], box2[:, 3])\n",
    "\n",
    "    # enclosing_diag_sq = (enc_x2 - enc_x1) ** 2 + (enc_y2 - enc_y1) ** 2 + 1e-6\n",
    "\n",
    "    # # Aspect ratio consistency (v)\n",
    "    # w1 = (pred[:, 2] - pred[:, 0]).clamp(1e-6)\n",
    "    # h1 = (pred[:, 3] - pred[:, 1]).clamp(1e-6)\n",
    "    # w2 = (box2[:, 2] - box2[:, 0]).clamp(1e-6)\n",
    "    # h2 = (box2[:, 3] - box2[:, 1]).clamp(1e-6)\n",
    "\n",
    "    # v = (4 / (torch.pi ** 2)) * torch.pow(\n",
    "    #     torch.atan(w2 / h2) - torch.atan(w1 / h1), 2\n",
    "    # )\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     S = 1 - iou\n",
    "    #     alpha = v / (S + v + 1e-6)\n",
    "\n",
    "    # ciou = iou - (center_dist_sq / enclosing_diag_sq) - alpha * v\n",
    "    return iou\n",
    "\n",
    "class IoULoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IoULoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        iou = bbox_ciou(pred, target)\n",
    "        loss = 1 - iou\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross entropy loss\n",
    "criterion_class = nn.CrossEntropyLoss()\n",
    "criterion_bbox_l1 = nn.SmoothL1Loss()\n",
    "criterion_bbox_iou = IoULoss()\n",
    "criterion_bbox_mse = nn.MSELoss()\n",
    "#criterion_bbox = torchvision.ops.complete_box_iou_loss\n",
    "weight_class = 0.0\n",
    "weight_l1 = 0.0\n",
    "weight_iou = 1.0\n",
    "weight_mse = 0.0\n",
    "\n",
    "#adam optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of epochs and early stopping\n",
    "epochs = 501\n",
    "early_stopping_patience = 500\n",
    "early_stopping_counter = 0\n",
    "\n",
    "#using validation loss as the best model\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "\n",
    "#arrays to save each metric during training\n",
    "train_loss_values = []\n",
    "train_acc_values = []\n",
    "\n",
    "val_loss_values = []\n",
    "val_acc_values = []\n",
    "\n",
    "#training loop\n",
    "for epoch in range(epochs):\n",
    "    #turn on training mode\n",
    "    model.train()\n",
    "\n",
    "    #storing loss and accuracy per batch\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "\n",
    "    for data, label, bbox in train_dl:\n",
    "        #moving data to the right device\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        bbox = np.transpose(bbox)\n",
    "        bbox = torch.tensor(bbox, dtype = torch.float32, device = device)\n",
    "        #print(bbox)\n",
    "\n",
    "        #clear gradients, forward pass, loss, backward and update\n",
    "        optimizer.zero_grad()\n",
    "        output_class, output_bbox = model(data)\n",
    "        loss_class = criterion_class (output_class, label)\n",
    "\n",
    "\n",
    "        #loss_bbox = criterion_bbox (output_bbox, bbox)\n",
    "        bbox_mask = (label == 1)\n",
    "        if bbox_mask.sum() > 0:\n",
    "            # Select only bbox predictions and targets for positive class samples\n",
    "            selected_output_bbox = output_bbox[bbox_mask]\n",
    "            selected_bbox = bbox[bbox_mask]\n",
    "\n",
    "            loss_bbox_iou = criterion_bbox_iou(selected_output_bbox, selected_bbox)\n",
    "            loss_bbox_l1 = criterion_bbox_l1(selected_output_bbox, selected_bbox)\n",
    "        else:\n",
    "            # No positive samples, so bbox loss is 0\n",
    "            loss_bbox_iou = torch.tensor(0.0, device=device)\n",
    "            loss_bbox_l1 = torch.tensor(0.0, device=device)\n",
    "\n",
    "\n",
    "        loss = (weight_class * loss_class) + (weight_iou * loss_bbox_iou) + (weight_l1 * loss_bbox_l1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #storing loss and accuracy over the batch\n",
    "        accuracy = (output_class.argmax(dim = 1) == label).float().mean().item()\n",
    "        train_losses.append(loss.item())\n",
    "        train_acc.append(accuracy)\n",
    "\n",
    "    \n",
    "    #average loss and acc over the epoch\n",
    "    current_epoch_loss = sum(train_losses) / len(train_losses)\n",
    "    current_epoch_acc = sum(train_acc) / len(train_acc)\n",
    "\n",
    "    #storing current epoch into overall loss and acc\n",
    "    train_loss_values.append(current_epoch_loss)\n",
    "    train_acc_values.append(current_epoch_acc)\n",
    "\n",
    "    #validation testing\n",
    "    #turn on evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    #storing loss and accuracy \n",
    "    val_losses = []\n",
    "    val_acc = []\n",
    "    \n",
    "    #disabling gradient tracking\n",
    "    with torch.no_grad():\n",
    "        for data, label, bbox in val_dl:\n",
    "            #moving data to the right device\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            bbox = np.transpose(bbox)\n",
    "            bbox = torch.tensor(bbox, dtype = torch.float32, device = device)\n",
    "            #print(bbox)\n",
    "\n",
    "            #running forward pass and calculating loss\n",
    "            val_class, val_bbox = model(data)\n",
    "            val_loss_class = criterion_class(val_class, label)\n",
    "\n",
    "\n",
    "            #val_loss_bbox = criterion_bbox (val_bbox, bbox)\n",
    "            bbox_mask = (label == 1)\n",
    "            if bbox_mask.sum() > 0:\n",
    "                selected_val_bbox = val_bbox[bbox_mask]\n",
    "                selected_bbox = bbox[bbox_mask]\n",
    "                #print(selected_val_bbox, selected_bbox)\n",
    "                val_loss_bbox_iou = criterion_bbox_iou(selected_val_bbox, selected_bbox)\n",
    "                val_loss_bbox_l1 = criterion_bbox_l1(selected_val_bbox, selected_bbox)\n",
    "            else:\n",
    "                val_loss_bbox_iou = torch.tensor(0.0, device=device)\n",
    "                val_loss_bbox_l1 = torch.tensor(0.0, device=device)\n",
    "\n",
    "\n",
    "            val_loss =  (weight_class * val_loss_class) + (weight_iou * val_loss_bbox_iou) + (weight_l1 * val_loss_bbox_l1)\n",
    "\n",
    "            #storing loss and accuracy\n",
    "            accuracy = (val_class.argmax(dim = 1) == label).float().mean().item()\n",
    "            val_losses.append(val_loss.item())\n",
    "            val_acc.append(accuracy)\n",
    "\n",
    "    #averaging loss and accuracy \n",
    "    current_epoch_val_loss = sum(val_losses) / len(val_losses)\n",
    "    current_epoch_val_acc = sum(val_acc) / len(val_acc)\n",
    "\n",
    "    #storing current epoch into overall\n",
    "    val_loss_values.append(current_epoch_val_loss)\n",
    "    val_acc_values.append(current_epoch_val_acc)\n",
    "\n",
    "    #checking for model improvement\n",
    "    if current_epoch_val_loss < best_val_loss:\n",
    "        #if best epoch, save it\n",
    "        torch.save(model.state_dict(), \"bounding_pistol.pth\")\n",
    "        \n",
    "        #best epoch information\n",
    "        best_val_loss = current_epoch_val_loss\n",
    "        best_epoch = epoch + 1\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    #output current epoch information\n",
    "    if epoch == 0 or epoch == 1 or epoch == 2 or epoch % 50 == 0:\n",
    "        print(f\"Epoch: {epoch+1}\")\n",
    "        print(f\"Training Accuracy: {current_epoch_acc:.3f}, Validation Accuracy: {current_epoch_val_acc:.3f}\")\n",
    "        print(f\"Training Loss: {current_epoch_loss:.3f}, Validation Loss: {current_epoch_val_loss:.3f}\")\n",
    "        print(f\"Current Best Epoch: {best_epoch}\\n\")\n",
    "\n",
    "    #stop if not improving\n",
    "    if early_stopping_counter >= early_stopping_patience:\n",
    "        print(f\"Stopping early after {early_stopping_patience} epochs with no improvement\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Model Training Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch count\n",
    "epoch_count = []\n",
    "for i in range(len(train_loss_values)):\n",
    "    epoch_count.append(i + 1)\n",
    "\n",
    "#setting figure size\n",
    "plt.figure(figsize = (8, 4))\n",
    "\n",
    "#plotting loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(epoch_count, train_loss_values, label='Training Loss')\n",
    "plt.plot(epoch_count, val_loss_values, label='Validation Loss', linestyle='--')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "#plotting accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(epoch_count, train_acc_values, label='Training Acc')\n",
    "plt.plot(epoch_count, val_acc_values, label='Validation Acc', linestyle='--')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "#showing figures\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading best saved model\n",
    "model.load_state_dict(torch.load('bounding_pistol.pth', weights_only = True))\n",
    "\n",
    "#set to evaluation mode\n",
    "model.eval() \n",
    "\n",
    "#saving images, label, bbox for visualization in next code block\n",
    "all_images, pred_labels, pred_bbox, true_labels, true_bbox = [], [], [], [], []\n",
    "\n",
    "#disabling gradient tracking\n",
    "with torch.no_grad(): \n",
    "    total_accuracy = 0.0\n",
    "    total_test_loss = 0.0\n",
    "    num_batches = len(test_dl)\n",
    "\n",
    "    for data, label, bbox in test_dl:\n",
    "        #loading data and labels to proper device\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        bbox = np.transpose(bbox)\n",
    "        bbox = torch.tensor(bbox, dtype = torch.float32, device = device)\n",
    "\n",
    "        #forward pass, loss, accuracy calculation\n",
    "        output_class, output_bbox = model(data)\n",
    "        loss_class = criterion_class(output_class, label)\n",
    "        loss_bbox = weight_iou * criterion_bbox_iou(output_bbox, bbox)\n",
    "        loss_bbox += weight_l1 * criterion_bbox_l1(output_bbox, bbox)\n",
    "        \n",
    "        loss = (weight_class * loss_class) + (loss_bbox)\n",
    "        accuracy = (output_class.argmax(dim = 1) == label).float().mean()\n",
    "\n",
    "        total_test_loss += loss.item()\n",
    "        total_accuracy += accuracy.item()\n",
    "\n",
    "        #storing individual data\n",
    "        for i in range(data.shape[0]):\n",
    "            all_images.append(data[i].clone().detach().cpu())\n",
    "            pred_labels.append(output_class[i].argmax(dim=0).item())\n",
    "            pred_bbox.append(output_bbox[i].clone().detach().cpu().tolist())\n",
    "            true_labels.append(label[i].item())\n",
    "            true_bbox.append(bbox[i].clone().detach().cpu().tolist())\n",
    "\n",
    "    # Calculate the average loss and accuracy over all batches\n",
    "    avg_loss = total_test_loss / num_batches\n",
    "    avg_accuracy = total_accuracy / num_batches\n",
    "\n",
    "    print(f\"Test Accuracy: {avg_accuracy:.3f}\")\n",
    "    print(f\"Test Loss : {avg_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(all_images)) #stored as Tensor\n",
    "#print(len(pred_labels))\n",
    "#print(len(true_labels))\n",
    "#print(len(pred_bbox), len(pred_bbox[0]))\n",
    "#print(len(true_bbox), len(true_bbox[0]))\n",
    "\n",
    "def show_image_with_bbox(image, true_label, pred_label, true_bbox, pred_bbox):\n",
    "    image = image.permute(1, 2, 0).numpy()  # Convert CHW to HWC format\n",
    "    image = (image * 255).astype(np.uint8)  # Rescale if needed\n",
    "    # Convert bbox coordinates to pixel values\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    #converting to pixels\n",
    "    true_bbox = [ int(true_bbox[0] ), int(true_bbox[1] ), int(true_bbox[2] ), int(true_bbox[3] )] \n",
    "    print(pred_bbox)\n",
    "    pred_bbox = [ int(pred_bbox[0] ), int(pred_bbox[1]), int(pred_bbox[2] ), int(pred_bbox[3] )]\n",
    "    #pred_bbox = [ int(pred_bbox[0] * width ), int(pred_bbox[1] * height), int(pred_bbox[2] * width ), int(pred_bbox[3] *height)]\n",
    "\n",
    "    #converting first two coords to top left point, instead of middle of the bounding box\n",
    "    #true_bbox[0] = int(true_bbox[0] - (true_bbox[2] / 2))\n",
    "    #true_bbox[1] = int(true_bbox[1] - (true_bbox[3] / 2))\n",
    "    #pred_bbox[0] = int(pred_bbox[0] - (pred_bbox[2] / 2))\n",
    "    #pred_bbox[0] = int(pred_bbox[1] - (pred_bbox[3] / 2))\n",
    "\n",
    "    print(pred_bbox)\n",
    "\n",
    "    if pred_bbox[0] < 0:\n",
    "        pred_bbox[0] = 0\n",
    "    if pred_bbox[1] < 0:\n",
    "        pred_bbox[1] = 0\n",
    "    if pred_bbox[2] < 0:\n",
    "        pred_bbox[2] = 0\n",
    "    if pred_bbox[3] < 0:\n",
    "        pred_bbox[3] = 0\n",
    "\n",
    "    if pred_bbox[0] > width:\n",
    "        pred_bbox[0] = width\n",
    "    if pred_bbox[1] > height:\n",
    "        pred_bbox[1] = height\n",
    "    if pred_bbox[2] > width:\n",
    "        pred_bbox[2] = width\n",
    "    if pred_bbox[3] > height:\n",
    "        pred_bbox[3] = height\n",
    "\n",
    "    print(pred_bbox)\n",
    "\n",
    "\n",
    "    # Draw actual bounding box (Green)\n",
    "    image = cv2.rectangle(image, \n",
    "                            (true_bbox[0], true_bbox[1]), \n",
    "                            (true_bbox[2], true_bbox[3]), \n",
    "                            (0, 255, 0), 2)\n",
    "\n",
    "    # Draw predicted bounding box (Red)\n",
    "    image = cv2.rectangle(image, \n",
    "                            (pred_bbox[0], pred_bbox[1]), \n",
    "                            (pred_bbox[2], pred_bbox[3]), \n",
    "                            (255, 0, 0), 2)\n",
    "\n",
    "    # Display labels\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"True Label: {true_label} | Pred Label: {pred_label}\", fontsize=10)\n",
    "    plt.show()\n",
    "#\n",
    "for i in range(len(all_images)):\n",
    "#for i in range(10):\n",
    "    if true_labels[i] == 1 or pred_labels[i] == 1 or 1:\n",
    "        show_image_with_bbox(\n",
    "            all_images[i],\n",
    "            true_labels[i],\n",
    "            pred_labels[i],\n",
    "            true_bbox[i],\n",
    "            pred_bbox[i]\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing pipeline process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def thermal_to_image( image_data):\n",
    "    hi = image_data[:, :, 0].astype(np.uint16)\n",
    "    lo = image_data[:, :, 1].astype(np.uint16)\n",
    "    raw_temp = hi * 256 + lo\n",
    "\n",
    "    #normalize for display (0–255)\n",
    "    normalized = cv2.normalize(raw_temp, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    normalized = normalized.astype(np.uint8)\n",
    "\n",
    "    #apply a color map for visibility\n",
    "    colored = cv2.applyColorMap(normalized, cv2.COLORMAP_JET)\n",
    "    return np.array(colored), np.array(normalized)\n",
    "\n",
    "def convert_raw_thermal(thermal_npy_path):\n",
    "    npy_file = np.load(thermal_npy_path)\n",
    "    image_data, thermal_data = np.array_split(npy_file, 2, axis = 1)\n",
    "    \n",
    "    #first section for image conversion\n",
    "    thermal_viewable, thermal_grayscale = thermal_to_image(image_data)\n",
    "    thermal_image, thermal_grayscale = thermal_viewable, thermal_grayscale\n",
    "    return thermal_grayscale\n",
    "\n",
    "\n",
    "def transform_image(test_image):\n",
    "    #shape and grayscale image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((384, 384)), \n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "\n",
    "    #transform, and change to have right dimensionality\n",
    "    test_image = transform(test_image)\n",
    "    test_image = test_image.unsqueeze(0) \n",
    "    return test_image\n",
    "\n",
    "def detect_and_bound_pistol(image):\n",
    "    image = Image.fromarray(image)\n",
    "    image = transform_image(image)\n",
    "\n",
    "   \n",
    "    image = image.to(device)\n",
    "    detected, pred_bbox = model(image)\n",
    "    \n",
    "    print(pred_bbox)\n",
    "\n",
    "\n",
    "test_image = convert_raw_thermal(r'C:\\Users\\tungu\\OneDrive\\Desktop\\Capstone\\Team-02-Capstone-Project\\Embedded System\\api\\venv\\localcache\\thermal_frame_appendix.npy')\n",
    "\n",
    "detect_and_bound_pistol(test_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
